\subsection{Detection}

\begin{figure}[h!]
    \centering
        \includegraphics[width=0.4\textwidth]{pictures/painting_detection/training-v3.png}
    \caption{YOLOv3 trained on our custom dataset}
    \label{fig:training-v3}
\end{figure}

\begin{figure}[]
    \centering
        \includegraphics[width=0.4\textwidth]{pictures/painting_detection/Frame2.jpg}
    \caption{Frame from video}
    \label{fig:a}
\end{figure}

\begin{figure}[]
    \centering
        \includegraphics[width=0.4\textwidth]{pictures/painting_detection/Adaptive_threshold.jpg}
    \caption{Adaptive threshold}
    \label{fig:b}
\end{figure}

\begin{figure}[]
    \centering
        \includegraphics[width=0.4\textwidth]{pictures/painting_detection/median_blur.jpg}
    \caption{Median blur}
    \label{fig:c}
\end{figure}

\begin{figure}[]
    \centering
        \includegraphics[width=0.4\textwidth]{pictures/painting_detection/erosion-dilation.jpg}
    \caption{Opening}
    \label{fig:d}
\end{figure}


\begin{figure}[]
    \centering
        \includegraphics[width=0.4\textwidth]{pictures/painting_detection/CCL.jpg}
    \caption{Connected Component Labeling}
    \label{fig:e}
\end{figure}

\begin{figure}[]
    \centering
        \includegraphics[width=0.4\textwidth]{pictures/painting_detection/bbox.jpg}
    \caption{Painting Detection}
    \label{fig:f}
\end{figure}




The detection of paintings and statues is done through a custom YOLOv3 neural network.\cite{yolov3}
Yolo is an architecture that provides a new approach to object detection, a single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation.
In order to use this neural network we have labeled 1343 images, 805 of them have been used to train the network, 269 for the validation set and 269 for the test set, providing a total number of 3733 labels. The images of paintings and statues were taken by the frame extracted by some videos recorded in the Galleria Estense, for the person class was instead used a mix of labeled images taken from the previous videos and a some images of people in another museum.
The training has been performed with darknet \cite{darknet} taking the mAP every 1000 iterations \ref{fig:training-v3}, this allowed us to take the best weights that weren't affected by overfitting.
 %(AP=98.61\% for the class 'painting ', AP = 95.34\% for the class 'statue', AP = 34.08\% for the class 'person' and a final recall of 93.97\%).
In the end we got a powerful neural network with high performance and capable of a good generalization (see: Table \ref{tab:detection_performance}).

\begin{table*}[ht]
    \centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\multicolumn{5}{|c|}{\textbf{Detection Performance}}       \\ \hline
\multicolumn{1}{|l|}{} & \textbf{Painting} & \textbf{Statue} & \textbf{Person} & \textbf{Overall} \\ \hline
\textbf{TP}        & 550     & 174     & 125     & 849     \\ \hline
\textbf{FP}        & 107     & 19      & 32      & 158     \\ \hline
\textbf{Precision} & 83,71\% & 90,16\% & 79,62\% & 84,31\% \\ \hline
\textbf{Recall}    & -       & -       & -       & 94\%    \\ \hline
\textbf{Average IoU}       & -       & -       & -       & 71\%    \\ \hline
\textbf{AP}        & 97,29\% & 98,61\% & 75,45\% & -       \\ \hline
\textbf{mAP}       & -       & -       & -       & 90,45\% \\ \hline
\end{tabular}
\caption{Detection Performance with YOLOv3}
    \label{tab:detection_performance}
\end{table*}



\subsubsection{Comparison with previous technique}
In a previous pipeline the detection was made without neural networks:
the image was at first time transformed in a grey level image then was preprocessed and the contours were computed. After the creation of the gray level image was applied an adaptive threshold (see:Figure \ref{fig:}) in order to take the significant edge, then was used a median blur for removing the noise which affected the image and the last processing was made by the erosion and the dilation.
The erosion and the dilation were combined together in order to create the opening, it was useful because with this technique the useless parts of the image were removed and the useful parts were increased.
When the preprocessing was terminated the pipeline had used the result to compute the foreground and the background with the Connected Component Labeling (see: \cite{Grana_ccl}), the result is showed in the image



and then we computed the edges with the Canny edge detector \cite{canny}. From the edges we took the significant borders that identified the paintings and finally draw the region of interest around the borders. Despite this method worked well in the main scenery, it wasn't able to adapt to strong luminance variation and to manage the presence of shadows.
The accuracy that was produced didn't satisfy the standard of the project, for this reason we choose to implement a neural network.

